{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539ff652-7e34-4ec1-a3d0-a7a72ad38dc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notes"
    }
   },
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Optimize reads\n",
    "# Define a schema (look at all the rows you're going to keep, and settle on a datatype)\n",
    "# Speeds up pipeline\n",
    "\n",
    "# Reading optimizations\n",
    "# Loop through the files reading them\n",
    "# cache it, (in memory for an hour or day)\n",
    "# trigger an action to actually perform the cache\n",
    "\n",
    "# Now you don't have to keep on rereading the dataframe by inserting the files\n",
    "# Important to df.cache to keep it! And then unpersist when you're done with that batch of\n",
    "# information\n",
    "\n",
    "# May make sense to repartition right on read? May not be worth it, something to think about.\n",
    "\n",
    "# Sampling: Test out files from different months to get a better idea of the data.\n",
    "\n",
    "# Regarding the actor ids with multiple logins:\n",
    "# After mentioning to Jordan how they can appear over 80 times, he said to just include the most recent one. This has been resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f08069-e242-4901-ba36-3e73b6b9ad40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### You are tasked with the following aggregations:\n",
    "\n",
    "- Data aggregated by type of GitHub event per hour\n",
    "- PushEvent data aggregated by ref type – whether the commit is on the main branch\n",
    "- Breakdown of events by type and number of commits per event\n",
    "- User activity should be aggregated so that a filterable chart can be populated with breakdowns of user activity by week or month.\n",
    "- Breakdown of activity by project – find a unique use case\n",
    "- Challenge: Based on the commit messages – breakdown the events by language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a703b117-88f6-4ccc-a419-43f7fffd653c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a3af5f-4a2c-4738-8e79-526157103213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, row_number, collect_set, count, max\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78247ae9-ef92-4827-b4b7-96e899c06346",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON schema"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType,MapType, BooleanType, LongType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# max read size of our partitions\n",
    "# default value is already 128 so we don't actually need this line\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 134217728) # 128 mb\n",
    "\n",
    "# Stanley: should we pass in a sample csv to ai to create our schema? We might have a lot of columns since it may explode everything out\n",
    "\n",
    "# Javi: I'm thinking the initial schema when reading in the files should be like this:\n",
    "# We realistically cannot specify a type for all the columns in the files, as the nesting for some entries will differ\n",
    "# We should only specify the types of the main columns\n",
    "\n",
    "# General schema for json\n",
    "schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('type', StringType(), True),\n",
    "    StructField('created_at', StringType(), True),\n",
    "    StructField('public', BooleanType(), True),\n",
    "    StructField('actor', StructType([\n",
    "        StructField('id', LongType(), True),\n",
    "        StructField('login', StringType(), True),\n",
    "    ]), True),\n",
    "    StructField('repo', StructType([\n",
    "        StructField('id', LongType(), True),\n",
    "        StructField('name', StringType(), True),\n",
    "    ]), True),\n",
    "    StructField('org', StructType([\n",
    "        StructField('id', LongType(), True),\n",
    "        StructField('login', StringType(), True),\n",
    "    ]), True),\n",
    "    StructField('payload', StructType([\n",
    "        StructField('action', StringType(), True),\n",
    "        StructField('size', LongType(), True),\n",
    "        StructField('distinct_size', LongType(), True),\n",
    "        StructField('ref', StringType(), True),\n",
    "        StructField('ref_type', StringType(), True),\n",
    "        StructField('pull_request', StructType([\n",
    "            StructField('closed_at', StringType(), True),\n",
    "            StructField('additions', LongType(), True),\n",
    "            StructField('deletions', LongType(), True),\n",
    "            StructField('changed_files', LongType(), True),\n",
    "            StructField('commits', LongType(), True),\n",
    "            StructField('merged', BooleanType(), True),\n",
    "            StructField('base', StructType([\n",
    "                StructField('ref', StringType(), True),\n",
    "                StructField('repo', StructType([\n",
    "                    StructField('language', StringType(), True),\n",
    "                    StructField('default_branch', StringType(), True),\n",
    "                    StructField('created_at', StringType(), True),\n",
    "                    StructField('fork', BooleanType(), True),\n",
    "                    StructField('private', BooleanType(), True),\n",
    "                ]), True),\n",
    "            ]), True),\n",
    "            StructField('head', StructType([\n",
    "                StructField('ref', StringType(), True),\n",
    "            ]), True),\n",
    "        ]), True),\n",
    "        StructField('issue', StructType([\n",
    "            StructField('closed_at', StringType(), True),\n",
    "            StructField('number', LongType(), True),\n",
    "        ]), True),\n",
    "        StructField('comment', StructType([\n",
    "            StructField('id', LongType(), True),\n",
    "        ]), True),\n",
    "        StructField('forkee', StructType([\n",
    "            StructField('id', LongType(), True),\n",
    "            StructField('full_name', StringType(), True),\n",
    "            StructField('language', StringType(), True),\n",
    "        ]), True),\n",
    "        StructField('member', StructType([\n",
    "            StructField('id', LongType(), True),\n",
    "            StructField('login', StringType(), True),\n",
    "        ]), True),\n",
    "    ]), True),\n",
    "])\n",
    "\n",
    "# Stanley: I agree, this is already a lot better than using inferSchema which is automatically True for reading jsons\n",
    "\n",
    "# After we read in the files, we remove the columns we don't need, and perhaps create a new schema and new df with the columns and their data type\n",
    "\n",
    "# That or we make every column into a string and fix the types later (Although this is more of a crazy idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6784e4-9363-48b4-afdc-35158d359168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32023ae4-9397-48e2-b6c6-9292b8c56a3d",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"org\":994,\"payload\":714,\"created_at\":168},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767826243164}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Load Data"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# files = [\n",
    "#     \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-01-15-*.json.gz\",\n",
    "#     \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-03-20-*.json.gz\",\n",
    "#     \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-05-10-*.json.gz\",\n",
    "#     \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-07-25-*.json.gz\",\n",
    "#     \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-09-05-*.json.gz\",\n",
    "#     \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-11-18-*.json.gz\",\n",
    "# ]\n",
    "\n",
    "\n",
    "path = \"abfss://gharchive-raw@20251124eyproject2.dfs.core.windows.net/2015-*-*-*.json.gz\"\n",
    "df = spark.read.schema(schema).json(path)\n",
    "# df_small = spark.read.schema(schema).json(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5cd53a-cc39-494a-8c71-3ae3d1d73b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Select Columns into new Dataframe From Flattened JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6160eaef-7025-49e5-8489-0570510d7dfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Selecting Columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# CHRIS\n",
    "\n",
    "# organized columns based on section \n",
    "silver_test = df.select(\n",
    "    col(\"id\").alias(\"event_id\"), # Primary key\n",
    "    col(\"type\").alias(\"event_type\"), # Event counts, branch analysis, basically everything\n",
    "    to_timestamp(\"created_at\").alias(\"created_at\"), # hourly breakdown, activity over time\n",
    "\n",
    "    # actor section\n",
    "    col(\"actor.id\").alias(\"actor_id\"),\n",
    "    col(\"actor.login\").alias(\"actor_login\"), # most active users\n",
    "\n",
    "    # repo section\n",
    "    col(\"repo.id\").alias(\"repo_id\"),\n",
    "    col(\"repo.name\").alias(\"repo_name\"), # Which repos have the most activity\n",
    "\n",
    "    # more repo stuff from PR base repo\n",
    "    col(\"payload.pull_request.base.repo.language\").alias(\"repo_language\"),\n",
    "    col(\"payload.pull_request.base.repo.default_branch\").alias(\"repo_default_branch\"),\n",
    "    col(\"payload.pull_request.base.repo.created_at\").alias(\"repo_created_at\"),\n",
    "    col(\"payload.pull_request.base.repo.fork\").alias(\"repo_is_fork\"),\n",
    "    col(\"payload.pull_request.base.repo.private\").alias(\"repo_is_private\"),\n",
    "\n",
    "    # org section\n",
    "    col(\"org.id\").alias(\"organization_id\"),\n",
    "    col(\"org.login\").alias(\"org_login\"), # rename in table - various different logins\n",
    "    \n",
    "    # Push Events\n",
    "    col(\"payload.size\").alias(\"commit_count\"), # Added; Commits per push, commit distribution\n",
    "    col(\"payload.distinct_size\").alias(\"distinct_size\"),\n",
    "\n",
    "    # Push, Create, and Delete\n",
    "    col(\"payload.ref\").alias(\"ref\"),\n",
    "    col(\"payload.ref_type\").alias(\"ref_type\"),\n",
    "\n",
    "    # PR section\n",
    "    col(\"payload.pull_request.closed_at\").alias(\"pr_closed_at\"),\n",
    "    col(\"payload.pull_request.additions\").alias(\"pr_additions\"),\n",
    "    col(\"payload.pull_request.deletions\").alias(\"pr_deletions\"),\n",
    "    col(\"payload.pull_request.changed_files\").alias(\"pr_changed_files\"),\n",
    "    col(\"payload.pull_request.commits\").alias(\"pr_commits\"),\n",
    "    col(\"payload.pull_request.base.ref\").alias(\"pr_base_ref\"),\n",
    "    col(\"payload.pull_request.head.ref\").alias(\"pr_head_ref\"),\n",
    "    col(\"payload.pull_request.merged\").alias(\"pr_merged\"),\n",
    "\n",
    "    # Issue Fields\n",
    "    col(\"payload.issue.closed_at\").alias(\"issue_closed_at\"),\n",
    "\n",
    "    # Issue Comment Event\n",
    "    col(\"payload.comment.id\").alias(\"comment_id\"),\n",
    "    col(\"payload.issue.number\").alias(\"issue_number\"),\n",
    "\n",
    "    # PR + Issue\n",
    "    col(\"payload.action\").alias(\"action\"), # Opened, closed, reopened, assigned, etc. might be useful. \n",
    "\n",
    "    # ForkEvent\n",
    "    col(\"payload.forkee.id\").alias(\"forkee_id\"),\n",
    "    col(\"payload.forkee.full_name\").alias(\"forkee_name\"),\n",
    "    col(\"payload.forkee.language\").alias(\"forkee_language\"),\n",
    "\n",
    "    # MemberEvent\n",
    "    col(\"payload.member.id\").alias(\"member_id\"),\n",
    "    col(\"payload.member.login\").alias(\"member_login\"),\n",
    "\n",
    ")\n",
    "\n",
    "    # Removed Event Types\n",
    "    # WatchEvent - payload.action is always \"started\" and no other significant data to capture\n",
    "    # PullRequestReviewCommentEvent - Same as WatchEvent\n",
    "    # CommitCommentEvent - Comments are just text descriptions, no analytical value, plus low volume\n",
    "    # GollumEvent - Wiki Edits, no analytical value\n",
    "    # ReleaseEvent - Too low volume\n",
    "    # PublicEvent - Too low volume\n",
    "\n",
    "    # Removed columns that are notable\n",
    "    # payload.pusher_type - literally all \"user\"\n",
    "    # payload.forkee.stargazers_count - ALL 0\n",
    "    # payload.forkee.watchers_count - ALL 0\n",
    "    # payload.pull_request.state - Redundant with action\n",
    "    # issue_created_at, pr_created_at - Redundant with created_at\n",
    "    # issue_state - Can get from action field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7592d2e-e09a-4dec-beaa-152ea8587c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Lookup Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7a62c5-d0bb-47f1-9971-d294241dcf6f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lookup Tables"
    }
   },
   "outputs": [],
   "source": [
    "# Creating lookup tables : Javi\n",
    "\n",
    "# For columns that don't have a specified id and will be made into lookup tables\n",
    "# You can assign a unique id for each value based on the row number to make a lookup table\n",
    "\n",
    "# Repo Table\n",
    "repo_lookup = (\n",
    "    silver_test\n",
    "    .select(\n",
    "        'repo_id',\n",
    "        'repo_name',\n",
    "        'repo_language',\n",
    "        'repo_default_branch',\n",
    "        'repo_created_at',\n",
    "        'repo_is_fork',\n",
    "        'repo_is_private'\n",
    "    )\n",
    "    .where(col(\"repo_language\").isNotNull()) # Keep rows w needed data\n",
    "    .dropDuplicates(['repo_id']) # THEN drop dupes\n",
    ")\n",
    "\n",
    "# Org Table\n",
    "org_lookup = (\n",
    "    silver_test\n",
    "    .select('organization_id', 'org_login')\n",
    "    .where(col(\"organization_id\").isNotNull()) # Don't want null values in lookup table\n",
    "    .dropDuplicates(['organization_id'])\n",
    ")\n",
    "\n",
    "# Main event table added to other events\n",
    "\n",
    "\n",
    "# After this, since we already have the IDs for the values in the dataframe, we would just drop the value columns, no need to do a join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd4e6ec8-dc19-4735-979a-eac0edb28fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Actor id,login Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5b2631-e9ce-42e3-9827-8733d3805e54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making the actor dimension table extracting only the 3 columns we need\n",
    "\n",
    "# ALL VALIDATED (just some variable names updated), IT WORKS WELL\n",
    "actor_dim_df = silver_test.select('actor_id','actor_login','created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb6ca0b-8265-4591-90fd-1294b88f5e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting a unique login for each id\n",
    "# We need to get the distinct id and login combinations, but we don't want the distinct timestamps as it would give us a larger row count than what it actually is\n",
    "# We group by id and login, and for each combination, we get the max timestamp for each group\n",
    "actor_login_ts_df = (\n",
    "    silver_test\n",
    "    .select(\n",
    "        col('actor_id').alias('actor_id'),\n",
    "        col('actor_login').alias('actor_login'),\n",
    "        col('created_at')\n",
    "    ).groupBy('actor_id','actor_login').agg(max('created_at').alias('created_at'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34a911f1-23d7-4615-b2d6-96b8bb668850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# We then partition by id, and order by the timestamp in descending order\n",
    "# We order by descending order, this is because we want to keep the latest timestamp\n",
    "w = Window.partitionBy(\"actor_id\").orderBy(col(\"created_at\").desc())\n",
    "\n",
    "# Since the latest timestamp will be row number 1 we filter our dataframe by row 1, the result is we keep the login with the latest timestamp\n",
    "latest_login_df = (\n",
    "    actor_login_ts_df\n",
    "    .withColumn(\"rn\", row_number().over(w))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"created_at\")\n",
    ")\n",
    "\n",
    "# After all of this, we can then remove the timestamp column and we have our lookup table!\n",
    "# How this could work for the whole dataset:\n",
    "    # Every time we ingest a new batch of data, since we'll probably be ingesting by date order, we can join the previous and new dataframes on ids after having run the code above, that way, we always have the latest login for the id\n",
    "    # That or we just specify lookup tables for each month or however we'll be ingesting data when we run the pipeline and run separate joins when we have all the lookup tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8074ea-097e-452c-8763-8471ae57b70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0795c37a-31a5-4a2c-adab-1c47f071d9cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fact Tables"
    }
   },
   "outputs": [],
   "source": [
    "# To create the specific events tables, we select the columns we need, and filter by event type\n",
    "\n",
    "# CHRIS\n",
    "\n",
    "# ALL VALIDATED\n",
    "''' Tables:\n",
    "fact_events\n",
    "fact_pull_request\n",
    "fact_issue\n",
    "fact_push_event\n",
    "fact_create_event\n",
    "fact_delete_event\n",
    "fact_fork_event\n",
    "fact_member_event\n",
    "fact_issue_comment\n",
    "'''\n",
    "\n",
    "# Main event Table\n",
    "fact_events = (\n",
    "    silver_test\n",
    "    .select(\n",
    "        'event_id',\n",
    "        'event_type', \n",
    "        'created_at',\n",
    "        'actor_id',\n",
    "        'repo_id',\n",
    "        'organization_id'\n",
    "    )\n",
    ")\n",
    "\n",
    "# PR Fact Table\n",
    "fact_pull_request = silver_test.select(\n",
    "    'event_id',\n",
    "    'action',\n",
    "    'pr_closed_at',\n",
    "    'pr_additions',\n",
    "    'pr_deletions',\n",
    "    'pr_changed_files',\n",
    "    'pr_commits',\n",
    "    'pr_base_ref',\n",
    "    'pr_head_ref',\n",
    "    'pr_merged'\n",
    ").where(silver_test.event_type == 'PullRequestEvent')\n",
    "\n",
    "# Issue Fact Table\n",
    "fact_issue = silver_test.select(\n",
    "    'event_id',\n",
    "    'action',\n",
    "    'issue_closed_at'\n",
    ").where(silver_test.event_type == 'IssuesEvent')\n",
    "\n",
    "# Push Event Fact Table\n",
    "fact_push_event = silver_test.select(\n",
    "    'event_id',\n",
    "    'commit_count',\n",
    "    'distinct_size',\n",
    "    'ref'\n",
    ").where(silver_test.event_type == 'PushEvent')\n",
    "\n",
    "# Create Event Fact Table\n",
    "fact_create_event = silver_test.select(\n",
    "    'event_id',\n",
    "    'ref',\n",
    "    'ref_type'\n",
    ").where(silver_test.event_type == 'CreateEvent')\n",
    "\n",
    "# Delete Event Fact Table\n",
    "fact_delete_event = silver_test.select(\n",
    "    'event_id',\n",
    "    'ref',\n",
    "    'ref_type'\n",
    ").where(silver_test.event_type == 'DeleteEvent')\n",
    "\n",
    "# Fork Event Fact Table\n",
    "fact_fork_event = silver_test.select(\n",
    "    'event_id',\n",
    "    'forkee_id',\n",
    "    'forkee_name',\n",
    "    'forkee_language'\n",
    ").where(silver_test.event_type == 'ForkEvent')\n",
    "\n",
    "# Member Event Fact Table\n",
    "fact_member_event = silver_test.select(\n",
    "    'event_id',\n",
    "    'action',\n",
    "    'member_id',\n",
    "    'member_login'\n",
    ").where(silver_test.event_type == 'MemberEvent')\n",
    "\n",
    "# Issue Fact Table \n",
    "fact_issue_comment = silver_test.select(\n",
    "    'event_id',\n",
    "    'comment_id',\n",
    "    'issue_number'\n",
    ").where(silver_test.event_type == 'IssueCommentEvent')\n",
    "\n",
    "# This can be done for any other event type that has specific information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1259098f-32df-4dd1-b0a9-4703bbbade74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Partition by EventType as 128 MB parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7dc72e7d-1212-4048-acc9-99cbfe20c8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"abfss://team3-project2@20251124eyproject2.dfs.core.windows.net/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f5e04c-961c-436c-94e1-6ac9ab1e8e94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Repartition & Write(FINAL)"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# I think we could add in all tables here since everything will be calculated anyways? \n",
    "# org_lookup, the actor table \n",
    "# intentionally added wrong_ so you dont run it again\n",
    "wrong_tables = {\n",
    "    \"fact_events\": fact_events,\n",
    "    \"fact_pull_request\": fact_pull_request,\n",
    "    \"fact_issue\": fact_issue,\n",
    "    \"fact_push_event\": fact_push_event,\n",
    "    \"fact_create_event\": fact_create_event,\n",
    "    \"fact_delete_event\": fact_delete_event,\n",
    "    \"fact_fork_event\": fact_fork_event,\n",
    "    \"fact_member_event\": fact_member_event,\n",
    "    \"fact_issue_comment\": fact_issue_comment,\n",
    "    \"repo_lookup\": repo_lookup,\n",
    "    \"actor_lookup\": latest_login_df,\n",
    "    \"org_lookup\": org_lookup\n",
    "}\n",
    "\n",
    "# NOTE: Don't need this anymore since scale_factor is redundant\n",
    "# CHANGE LATER since we'll be processing by month\n",
    "# create a dict later to calculate processed files?\n",
    "# days_in_month = {\n",
    "#     \"jan\": 31, \"feb\": 28, \"mar\": 31, \"apr\": 30,\n",
    "#     \"may\": 31, \"jun\": 30, \"jul\": 31, \"aug\": 31,\n",
    "#     \"sep\": 30, \"oct\": 31, \"nov\": 30, \"dec\": 31\n",
    "# }\n",
    "# days * 24 hours\n",
    "# files_in_month = days_in_month[month] * 24\n",
    "# Don't need scale factor anymore, we are estimating sum of bytes in each cleaned dataframe of the month instead\n",
    "# scale_factor = 8760 / 144\n",
    "\n",
    "# 128MB target size\n",
    "TARGET_PARTITION_SIZE = 128 * 1024 * 1024\n",
    "# NOTE: conversion between cleaned dataframe to parquet\n",
    "# after running test, changed 0.25 to 0.117\n",
    "# 102 MB / 0.25 = 408 MB\n",
    "# 47.77 MB in container / 408\n",
    "parquet_compression = 0.117 # estimate is closer to actual size now\n",
    "\n",
    "# where to write to\n",
    "path_to_silver = \"abfss://team3-project2@20251124eyproject2.dfs.core.windows.net/silver/\"\n",
    "\n",
    "# converts a single row in the cleaned dataframe to json, then computes size\n",
    "# TEST\n",
    "def size_in_bytes(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    total = pdf.apply(lambda row: row.to_json().encode(\"utf-8\").__len__(), axis=1).sum()\n",
    "    return pd.DataFrame({\"total_bytes\": [total]})\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    # Step 1: Estimate sample size in bytes with the function above\n",
    "    # TEST\n",
    "    # Sample 1% of rows instead of the entire dataframe, used to calculate avg row size\n",
    "    df_sample = df.sample(fraction=0.01, seed=42)\n",
    "    # Calculates total bytes in the sample\n",
    "    total_bytes_sample = df_sample.mapInPandas(\n",
    "        lambda pdf_iter: (size_in_bytes(pdf) for pdf in pdf_iter),\n",
    "        schema=\"total_bytes long\"\n",
    "    ).agg({\"total_bytes\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "    # Gets average row size using the sample\n",
    "    num_sample_rows = df_sample.count()\n",
    "    avg_row_size = total_bytes_sample / num_sample_rows\n",
    "    \n",
    "    # Step 2: Scales to the full dataframe -> parquet\n",
    "    # total_rows = df.count() # this would take too long\n",
    "    total_rows_estimate = int(num_sample_rows / 0.01) # this is a bit off but better since we are only using 1% of dataframe\n",
    "    # total_size_estimate_bytes of dataframe as parquet\n",
    "    total_size_estimate_bytes = avg_row_size * total_rows_estimate * parquet_compression # removed: * scale_factor\n",
    "    \n",
    "    # Step 3: Computes the number of 128MB partitions we have\n",
    "    rows_per_partition = TARGET_PARTITION_SIZE / (avg_row_size * parquet_compression)\n",
    "    # if num of partition is 0, then default to 1\n",
    "    # this will happen if dataframe is less than 128MB\n",
    "    num_partitions = int(total_rows_estimate / rows_per_partition)\n",
    "    if(num_partitions == 0):\n",
    "        num_partitions = 1\n",
    "    \n",
    "    # Step 4: Print estimates, Use this for logging?\n",
    "    #print(f\"Table: {table_name}\\n\") # maybe add in the month the table is from?\n",
    "    #print(f\"Estimated total rows: {int(total_rows_estimate)}\\n\")\n",
    "    #print(f\"Estimated total size as parquet (GB): {total_size_estimate_bytes / (1024**3):.2f}\\n\")\n",
    "    #print(f\"Recommended partitions for ~128MB files: {num_partitions}\\n\\n\")\n",
    "\n",
    "    # NOTE: Step 5: Writes to the disk, need to add month to file path\n",
    "    # ex: path_to_silver + table_name + month + \"/\"\n",
    "    df.repartition(num_partitions).write.mode(\"overwrite\").parquet(path_to_silver + table_name + \"/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd9f1d8-2d32-42be-a0b5-4fac3f2efd92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69c8609f-5bab-421e-8c04-d6e5b20a4707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df_small.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b4c502-4921-41cd-acf3-871d0291086e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing Below"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql.functions import collect_set,count\\n\\nactor_dim_df     .groupBy(\"actor_id\")     .agg(\\n        collect_set(\"actor_login\").alias(\"actor_logins\"),\\n        count(\"*\").alias(\"row_count\")\\n    )     .filter(\"row_count > 1\")     .orderBy(\"actor_id\")     .show(truncate=False)\\n\\nSOLVED, BY JAVI\\n\\nStill need to figure out how to deal with multi actor_logins for same actor_id\\n- most recent one\\n- column for each actor_login, explode\\n- if more than 2 actor logins show up, maybe a column for original name and one for current name?\\n\\nOriginal:\\n|actor_id|actor_logins             |row_count\\n|82479   |[Laith, LaithLaith]      |2 \\n\\nI don\\'t think we need to change any past rows, just future ones when a new actor_login appears (username change)\\n\\nSo maybe it can look like this? \\nWe can also just ignore original_actor_login and stick with the current one since we aren\\'t doing too much analytics on actor_login\\n\\nOption 1:\\n|actor_id   |original_actor_login   |current_actor_login   |row_count\\n|82479      |Laith                  |Laith                 |2 \\n|82479      |Laith                  |LaithLaith            |2                  <- this event happened after new actor_login is added\\n\\nOption 2:\\n|actor_id   |current_actor_login   |row_count\\n|82479      |Laith                 |2 \\n|82479      |LaithLaith            |2                    <- this event happened after new actor_login is added\\n\\nOption 3:\\n'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "from pyspark.sql.functions import collect_set,count\n",
    "\n",
    "actor_dim_df \\\n",
    "    .groupBy(\"actor_id\") \\\n",
    "    .agg(\n",
    "        collect_set(\"actor_login\").alias(\"actor_logins\"),\n",
    "        count(\"*\").alias(\"row_count\")\n",
    "    ) \\\n",
    "    .filter(\"row_count > 1\") \\\n",
    "    .orderBy(\"actor_id\") \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "SOLVED, BY JAVI\n",
    "\n",
    "Still need to figure out how to deal with multi actor_logins for same actor_id\n",
    "- most recent one\n",
    "- column for each actor_login, explode\n",
    "- if more than 2 actor logins show up, maybe a column for original name and one for current name?\n",
    "\n",
    "Original:\n",
    "|actor_id|actor_logins             |row_count\n",
    "|82479   |[Laith, LaithLaith]      |2 \n",
    "\n",
    "I don't think we need to change any past rows, just future ones when a new actor_login appears (username change)\n",
    "\n",
    "So maybe it can look like this? \n",
    "We can also just ignore original_actor_login and stick with the current one since we aren't doing too much analytics on actor_login\n",
    "\n",
    "Option 1:\n",
    "|actor_id   |original_actor_login   |current_actor_login   |row_count\n",
    "|82479      |Laith                  |Laith                 |2 \n",
    "|82479      |Laith                  |LaithLaith            |2                  <- this event happened after new actor_login is added\n",
    "\n",
    "Option 2:\n",
    "|actor_id   |current_actor_login   |row_count\n",
    "|82479      |Laith                 |2 \n",
    "|82479      |LaithLaith            |2                    <- this event happened after new actor_login is added\n",
    "\n",
    "Option 3:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba5d536-c024-4794-bc78-cdafd556e319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== actor_lookup ===\nRow count: 4,331,563\n+--------+----------------+\n|actor_id|     actor_login|\n+--------+----------------+\n|     159|technicalpickles|\n|     170|        rbazinet|\n|     314|        zmoazeni|\n+--------+----------------+\nonly showing top 3 rows\n\n\n=== org_lookup ===\nRow count: 281,479\n+---------------+--------------------+\n|organization_id|           org_login|\n+---------------+--------------------+\n|         362782|              yzalis|\n|       10242389|travis-infrastruc...|\n|       10772219|          clickberry|\n+---------------+--------------------+\nonly showing top 3 rows\n\n\n=== repo_lookup ===\nRow count: 726,388\n+--------+--------------------+-------------+-------------------+--------------------+------------+---------------+\n| repo_id|           repo_name|repo_language|repo_default_branch|     repo_created_at|repo_is_fork|repo_is_private|\n+--------+--------------------+-------------+-------------------+--------------------+------------+---------------+\n|37734226|TechnologyAdvice/...|   JavaScript|             master|2015-06-19T16:47:19Z|       false|          false|\n|17224514|    ehcache/ehcache3|         Java|             master|2014-02-26T20:39:16Z|       false|          false|\n|44323315|department-of-vet...|         Ruby|             master|2015-10-15T14:32:04Z|       false|          false|\n+--------+--------------------+-------------+-------------------+--------------------+------------+---------------+\nonly showing top 3 rows\n\n\n=== fact_events ===\nRow count: 212,222,001\n+----------+-----------------+-------------------+--------+--------+---------------+\n|  event_id|       event_type|         created_at|actor_id| repo_id|organization_id|\n+----------+-----------------+-------------------+--------+--------+---------------+\n|3195047166|        PushEvent|2015-09-30 15:00:00|10269519|31305253|           NULL|\n|3195047311|        PushEvent|2015-09-30 15:00:03|14296571|43085163|           NULL|\n|3195047567|IssueCommentEvent|2015-09-30 15:00:07| 3043834|30751834|       10990788|\n+----------+-----------------+-------------------+--------+--------+---------------+\nonly showing top 3 rows\n\n\n=== fact_push_event ===\nRow count: 104,635,436\n+----------+------------+-------------+--------------------+\n|  event_id|commit_count|distinct_size|                 ref|\n+----------+------------+-------------+--------------------+\n|3148728554|           1|            1|refs/heads/refact...|\n|3148728669|           2|            2|   refs/heads/master|\n|3148728814|           1|            1|   refs/heads/master|\n+----------+------------+-------------+--------------------+\nonly showing top 3 rows\n\n\n=== fact_create_event ===\nRow count: 28,409,818\n+----------+------------+--------+\n|  event_id|         ref|ref_type|\n+----------+------------+--------+\n|3471362483|      master|  branch|\n|3471362498|1-mouse-over|  branch|\n|3471362517|      CF-915|  branch|\n+----------+------------+--------+\nonly showing top 3 rows\n\n\n=== fact_delete_event ===\nRow count: 4,477,514\n+----------+--------------------+--------+\n|  event_id|                 ref|ref_type|\n+----------+--------------------+--------+\n|3454196844|selective-cache-c...|  branch|\n|3454196976|feat/deployForBro...|  branch|\n|3454196988|      6290_liked_map|  branch|\n+----------+--------------------+--------+\nonly showing top 3 rows\n\n\n=== fact_pull_request ===\nRow count: 10,842,254\n+----------+------+--------------------+------------+------------+----------------+----------+-----------+-----------+---------+\n|  event_id|action|        pr_closed_at|pr_additions|pr_deletions|pr_changed_files|pr_commits|pr_base_ref|pr_head_ref|pr_merged|\n+----------+------+--------------------+------------+------------+----------------+----------+-----------+-----------+---------+\n|3195047243|closed|2015-09-30T15:00:02Z|         812|         915|              80|         4|     master|      ps-89|     true|\n|3195047325|closed|2015-09-30T15:00:02Z|           1|           1|               1|         1|     master|    patch-1|     true|\n|3195047620|closed|2015-09-30T15:00:07Z|           3|           0|               1|         1|     master|readme-edit|     true|\n+----------+------+--------------------+------------+------------+----------------+----------+-----------+-----------+---------+\nonly showing top 3 rows\n\n\n=== fact_issue ===\nRow count: 9,902,698\n+----------+------+--------------------+\n|  event_id|action|     issue_closed_at|\n+----------+------+--------------------+\n|3454196876|closed|2015-12-17T15:00:00Z|\n|3454196923|closed|2015-12-17T15:00:01Z|\n|3454196948|closed|2015-12-17T15:00:01Z|\n+----------+------+--------------------+\nonly showing top 3 rows\n\n\n=== fact_fork_event ===\nRow count: 7,038,657\n+----------+---------+--------------------+---------------+\n|  event_id|forkee_id|         forkee_name|forkee_language|\n+----------+---------+--------------------+---------------+\n|3216635511| 43827546|     nasomi/darkstar|           NULL|\n|3216635737| 43827550|UgnisSoftware/flu...|           NULL|\n|3216635790| 43827551|ferpinand/meteor-...|           NULL|\n+----------+---------+--------------------+---------------+\nonly showing top 3 rows\n\n\n=== fact_member_event ===\nRow count: 1,175,125\n+----------+------+---------+------------+\n|  event_id|action|member_id|member_login|\n+----------+------+---------+------------+\n|3216637089| added| 15016993|    dengjili|\n|3216637156| added|  1190054|  pawciobiel|\n|3216637633| added|  1243959| kraenhansen|\n+----------+------+---------+------------+\nonly showing top 3 rows\n\n\n=== fact_issue_comment ===\nRow count: 19,152,422\n+----------+----------+------------+\n|  event_id|comment_id|issue_number|\n+----------+----------+------------+\n|3454196814| 165476409|          30|\n|3454196846| 165476410|         519|\n|3454196851| 165476411|         127|\n+----------+----------+------------+\nonly showing top 3 rows\n\n\n"
     ]
    }
   ],
   "source": [
    "path_to_silver = \"abfss://team3-project2@20251124eyproject2.dfs.core.windows.net/silver/\"\n",
    "\n",
    "tables = [\n",
    "    \"actor_lookup\",\n",
    "    \"org_lookup\", \n",
    "    \"repo_lookup\",\n",
    "    \"fact_events\",\n",
    "    \"fact_push_event\",\n",
    "    \"fact_create_event\",\n",
    "    \"fact_delete_event\",\n",
    "    \"fact_pull_request\",\n",
    "    \"fact_issue\",\n",
    "    \"fact_fork_event\",\n",
    "    \"fact_member_event\",\n",
    "    \"fact_issue_comment\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"=== {table} ===\")\n",
    "    df = spark.read.parquet(path_to_silver + table + \"/\")\n",
    "    print(f\"Row count: {df.count():,}\")\n",
    "    df.show(3)\n",
    "    print(\"\\n\")\n",
    "\n",
    "ALL IS WELL"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_to_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}